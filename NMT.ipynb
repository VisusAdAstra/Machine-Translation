{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.attention as attention\n",
    "importlib.reload(attention);\n",
    "import src.models as model\n",
    "importlib.reload(model)\n",
    "import src.utils as util\n",
    "importlib.reload(util);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-word mapping\n",
    "target_vocab_size_en = 400\n",
    "target_vocab_size_fr = 600\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\" --input=data/small_vocab_en --model_type=unigram --hard_vocab_limit=false\" +\n",
    "    f\" --model_prefix=data/en --vocab_size={target_vocab_size_en}\")\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\" --input=data/small_vocab_fr --model_type=unigram --hard_vocab_limit=false\" +\n",
    "    f\" --model_prefix=data/fr --vocab_size={target_vocab_size_fr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global data\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.Load(os.path.join(\"data\", 'en.model'))\n",
    "\n",
    "sp_fr = spm.SentencePieceProcessor()\n",
    "sp_fr.Load(os.path.join(\"data\", 'fr.model'))\n",
    "\n",
    "\n",
    "file = open(os.path.join('data', 'small_vocab_en'), 'r', encoding='utf-8')\n",
    "en_text = file.read().split(\"\\n\")\n",
    "file = open(os.path.join('data', 'small_vocab_fr'), 'r', encoding='utf-8')\n",
    "fr_text = file.read().split(\"\\n\")\n",
    "\n",
    "train_en_X = []\n",
    "train_fr_X = []\n",
    "train_fr_Y = []\n",
    "\n",
    "en_max_len = 0\n",
    "fr_max_len = 0\n",
    "\n",
    "vocab_size_en = sp_en.GetPieceSize() #-1\n",
    "vocab_size_fr = sp_fr.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Tokenizer\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bert(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "# source_tokenizer = [tokenize_bert(ele) for ele in source]\n",
    "# source_tensor = source_tokenizer\n",
    "# print(source_tokenizer[0])\n",
    "\n",
    "# target_tokenizer = [tokenize_bert(ele) for ele in target]\n",
    "# target_tensor = target_tokenizer\n",
    "# print(target_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(source_tensor, target_tensor, test_size=0.2, random_state=0)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# Show length\n",
    "# print(len(X_train), len(y_train), len(X_val), len(y_val))\n",
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the source and target tokens and post pad them\n",
    "# Assuming three extra tokens: <end>: #vocab_size_en | #vocab_size_fr,\n",
    "# <empty>: #vocab_size_en+1 | #vocab_size_fr+1, and <start>: #vocab_size_fr+2\n",
    "\n",
    "end_token_id_en = vocab_size_en\n",
    "empty_token_id_en = vocab_size_en + 1\n",
    "end_token_id_fr = vocab_size_fr\n",
    "empty_token_id_fr = vocab_size_fr + 1\n",
    "start_token_id_fr = vocab_size_fr + 2\n",
    "\n",
    "# The input text only needs two extra tokens while the output needs 3\n",
    "vocab_size_en = vocab_size_en + 2\n",
    "vocab_size_fr = vocab_size_fr + 3\n",
    "\n",
    "\n",
    "for i in range(len(en_text)):\n",
    "  en_seq = sp_en.EncodeAsIds(en_text[i].strip()) + [end_token_id_en]\n",
    "  en_max_len = max(en_max_len, len(en_seq))\n",
    "  train_en_X.append(en_seq)\n",
    "\n",
    "  fr_seq = sp_fr.EncodeAsIds(fr_text[i].strip()) + [end_token_id_fr]\n",
    "  fr_max_len = max(fr_max_len, len(fr_seq))\n",
    "  train_fr_X.append(fr_seq)\n",
    "\n",
    "# Cleaning up the memory, work with subword\n",
    "en_text = []\n",
    "fr_text = []\n",
    "\n",
    "# Padding all the samples with <empty> token to make them all of the same length\n",
    "# equal to the longest one\n",
    "train_en_X = pad_sequences(train_en_X, maxlen=en_max_len,\n",
    "                           padding=\"post\", value=empty_token_id_en)\n",
    "# maxlen is fr_max_len+1 since we need to accomodate for <start>\n",
    "train_fr_X = pad_sequences(train_fr_X, maxlen=fr_max_len+1,\n",
    "                           padding=\"post\", value=empty_token_id_fr)\n",
    "\n",
    "# Converting the train_fr_Y to a one-hot vector needed by the training phase as\n",
    "# the output\n",
    "train_fr_Y = to_categorical(train_fr_X, num_classes=vocab_size_fr)\n",
    "\n",
    "# Moving the last <empty> to the first position in each input sample\n",
    "train_fr_X = np.roll(train_fr_X, 1, axis=-1)\n",
    "# Changing the first token in each input sample to <start>\n",
    "train_fr_X[:, 0] = start_token_id_fr\n",
    "\n",
    "fr_max_len = fr_max_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Input Language; index to word mapping\")\n",
    "# util.convert(source_tokenizer, X_train[0])\n",
    "# print ()\n",
    "# print (\"Target Language; index to word mapping\")\n",
    "# util.convert(target_tokenizer, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder input (English)\n",
    "hidden_dim = 128\n",
    "input_en = Input(batch_shape=(None, en_max_len), name='input_en')\n",
    "\n",
    "# English embedding layer\n",
    "embedding_en = layers.Embedding(vocab_size_en, hidden_dim, name='embedding_en')\n",
    "embedded_en = embedding_en(input_en)\n",
    "\n",
    "# Encoder RNN (LSTM) layer\n",
    "encoder_lstm = layers.Bidirectional(\n",
    "                  layers.LSTM(hidden_dim,\n",
    "                              return_sequences=True, return_state=True),\n",
    "                  name=\"encoder_lstm\")\n",
    "(encoded_en,\n",
    "  forward_h_en, forward_c_en,\n",
    "  backward_h_en, backward_c_en) = encoder_lstm(embedded_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder input (French)\n",
    "input_fr = Input(batch_shape=(None, None), name='input_fr')\n",
    "\n",
    "# English embedding layer\n",
    "embedding_fr = layers.Embedding(vocab_size_fr, hidden_dim, name='embedding_fr')\n",
    "embedded_fr = embedding_fr(input_fr)\n",
    "\n",
    "state_h_en = layers.concatenate([forward_h_en, backward_h_en])\n",
    "state_c_en = layers.concatenate([forward_c_en, backward_c_en])\n",
    "\n",
    "# Decoder RNN (LSTM) layer\n",
    "decoder_lstm = layers.LSTM(hidden_dim * 2, return_sequences=True,\n",
    "                           return_state=True, name=\"decoder_lstm\")\n",
    "(encoded_fr,\n",
    "  forward_h_fr, forward_c_fr) = decoder_lstm(embedded_fr,\n",
    "                 initial_state=[state_h_en, state_c_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention layer\n",
    "attention_layer = attention.AttentionLayer(name='attention_layer')\n",
    "attention_out, attention_states = attention_layer({\"values\": encoded_en,\n",
    "                                                   \"query\": encoded_fr})\n",
    "\n",
    "# Concatenating the decoder output with attention output\n",
    "rnn_output = layers.concatenate([encoded_fr, attention_out], name=\"rnn_output\")\n",
    "\n",
    "# Dense layer\n",
    "dense_layer0 = layers.Dense(2048, activation='relu', name='dense_0')\n",
    "dl0 = dense_layer0(rnn_output)\n",
    "\n",
    "dense_layer1 = layers.Dense(1024, activation='relu', name='dense_1')\n",
    "dl1 = dense_layer1(dl0)\n",
    "\n",
    "dense_layer2 = layers.Dense(512, activation='relu', name='dense_2')\n",
    "dl2 = dense_layer2(dl1)\n",
    "\n",
    "dl2 = layers.Dropout(0.4)(dl2)\n",
    "\n",
    "dense_layer3 = layers.Dense(vocab_size_fr, activation='softmax', name='dense_3')\n",
    "dense_output = dense_layer3(dl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_en (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_en (Embedding)        (None, 50, 128)      44672       input_en[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_fr (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (Bidirectional)    [(None, 50, 256), (N 263168      embedding_en[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_fr (Embedding)        (None, None, 128)    65024       input_fr[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][3]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           encoder_lstm[0][2]               \n",
      "                                                                 encoder_lstm[0][4]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  394240      embedding_fr[0][0]               \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      decoder_lstm[0][0]               \n",
      "                                                                 encoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "rnn_output (Concatenate)        (None, None, 512)    0           decoder_lstm[0][0]               \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, None, 2048)   1050624     rnn_output[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1024)   2098176     dense_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 512)    524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 512)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 508)    260604      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,832,636\n",
      "Trainable params: 4,832,636\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_en (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_en (Embedding)        (None, 50, 128)      44672       input_en[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (Bidirectional)    [(None, 50, 256), (N 263168      embedding_en[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][3]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           encoder_lstm[0][2]               \n",
      "                                                                 encoder_lstm[0][4]               \n",
      "==================================================================================================\n",
      "Total params: 307,840\n",
      "Trainable params: 307,840\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_fr (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_fr (Embedding)        (None, None, 128)    65024       input_fr[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_h (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_c (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  394240      embedding_fr[0][0]               \n",
      "                                                                 input_h[0][0]                    \n",
      "                                                                 input_c[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_encoded_en (InputLayer)   [(None, 50, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 256),  131328      decoder_lstm[1][0]               \n",
      "                                                                 input_encoded_en[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "generative_output (Concatenate) (None, None, 512)    0           decoder_lstm[1][0]               \n",
      "                                                                 attention_layer[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_0 (Dense)                 (None, None, 2048)   1050624     generative_output[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1024)   2098176     dense_0[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 512)    524800      dense_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 508)    260604      dense_2[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,524,796\n",
      "Trainable params: 4,524,796\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "training_model = models.Model([input_en, input_fr], dense_output)\n",
    "training_model.summary()\n",
    "\n",
    "training_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=[model.MaskedCategoricalAccuracy(empty_token_id_fr),\n",
    "                                model.ExactMatchedAccuracy(empty_token_id_fr)])\n",
    "\n",
    "# Generative models\n",
    "encoder_model = models.Model([input_en],\n",
    "                             [encoded_en,\n",
    "                              state_h_en, state_c_en])\n",
    "encoder_model.summary()\n",
    "\n",
    "\n",
    "# The decoder model, to generate the French tokens (in integer form)\n",
    "input_h = layers.Input(batch_shape=(None, hidden_dim * 2),\n",
    "                       name='input_h')\n",
    "input_c = layers.Input(batch_shape=(None, hidden_dim * 2),\n",
    "                       name='input_c')\n",
    "\n",
    "(decoder_output,\n",
    "  output_h,\n",
    "  output_c) = decoder_lstm(embedded_fr,\n",
    "                           initial_state=[input_h, input_c])\n",
    "\n",
    "input_encoded_en = layers.Input(batch_shape=(None, en_max_len, hidden_dim * 2),\n",
    "                                name='input_encoded_en')\n",
    "\n",
    "attention_out, attention_state = attention_layer({\"values\": input_encoded_en,\n",
    "                                                  \"query\": decoder_output})\n",
    "\n",
    "generative_output = layers.concatenate([decoder_output,\n",
    "                                        attention_out],\n",
    "                                       name=\"generative_output\")\n",
    "\n",
    "g0 = dense_layer0(generative_output)\n",
    "g1 = dense_layer1(g0)\n",
    "g2 = dense_layer2(g1)\n",
    "dense_output = dense_layer3(g2)\n",
    "\n",
    "decoder_model = models.Model([input_encoded_en, input_fr,\n",
    "                              input_h, input_c],\n",
    "                             [dense_output, attention_state,\n",
    "                              output_h, output_c])\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7c3cbc9069bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                        restore_best_weights = True)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m history = training_model.fit(x=[train_en_X, train_fr_X], y=train_fr_Y, batch_size=786,\n\u001b[0m\u001b[0;32m     12\u001b[0m                              \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                              workers=3, use_multiprocessing=True, callbacks=[pocket])\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1130\u001b[0m                 _r=1):\n\u001b[0;32m   1131\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2969\u001b[0m       (graph_function,\n\u001b[0;32m   2970\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2971\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2972\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1945\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1946\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1947\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1948\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1949\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    557\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "checkpoint_dir = 'save'\n",
    "\n",
    "pocket = EarlyStopping(monitor='val_exact_matched_accuracy', min_delta=0.001,\n",
    "                       patience=10, verbose=1, mode='max',\n",
    "                       restore_best_weights = True)\n",
    "\n",
    "history = training_model.fit(x=[train_en_X, train_fr_X], y=train_fr_Y, batch_size=786,\n",
    "                             epochs=100, verbose=1, validation_split=0.3, shuffle=True,\n",
    "                             workers=5, use_multiprocessing=True, callbacks=[pocket])\n",
    "\n",
    "training_model.save_weights(os.path.join(checkpoint_dir, \"nmt.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "training_model.load_weights(os.path.join(checkpoint_dir, \"nmt.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "util.plot_training(history)\n",
    "\n",
    "results = training_model.evaluate(x=[train_en_X, train_fr_X], y=train_fr_Y,\n",
    "                                  batch_size=786, verbose=1,\n",
    "                                  workers=1, use_multiprocessing=False)\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test masked categorical accuracy:', results[1])\n",
    "print('Test exact matched accuracy:', results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# The input English string\n",
    "english_string = \"the united states is never freezing during november , but the united states is sometimes rainy in winter .\"\n",
    "\n",
    "# First, let's tokenize the Eglish string, then pad it\n",
    "english_tokens = sp_en.EncodeAsIds(english_string.strip()) + [end_token_id_en]\n",
    "english_tokens = pad_sequences([english_tokens], maxlen=en_max_len,\n",
    "                               padding=\"post\", value=empty_token_id_en)\n",
    "\n",
    "# The encoder, we only need to use it once per each English string\n",
    "(encoded_en_test,\n",
    "  state_h_en_test, state_c_en_test) = encoder_model.predict(english_tokens)\n",
    "\n",
    "# In order to find a better translation, we are using Beam search\n",
    "beam_search_list = [{\n",
    "  \"decoder_input\": {\n",
    "    \"input_encoded_en\": encoded_en_test,\n",
    "    \"input_fr\": np.array([[start_token_id_fr]]),\n",
    "    \"input_h\": state_h_en_test,\n",
    "    \"input_c\": state_c_en_test\n",
    "  },\n",
    "  \"score\": 0.0,\n",
    "  \"parent_node\": None,\n",
    "  \"depth\": 0,\n",
    "  \"attention_weights\": None,\n",
    "}]\n",
    "ended_branches = []\n",
    "\n",
    "beam_size = 10\n",
    "\n",
    "# We are generating up to fr_max_len tokens\n",
    "for i in range(fr_max_len):\n",
    "  new_beam_candidates = []\n",
    "  # Predict the next token for each member of the list\n",
    "  for beam in beam_search_list:\n",
    "    # Use the decoder to predict the next token using the previously\n",
    "    # predicted token\n",
    "    (output,\n",
    "      attention_out,\n",
    "      state_h_en_test,\n",
    "      state_c_en_test) = decoder_model.predict(beam[\"decoder_input\"])\n",
    "    # Find the top beam_size candidates\n",
    "    top_k = np.argpartition(output[0, 0, :], -beam_size)[-beam_size:]\n",
    "    # For each candidate, put it in the list to predict the next token for it\n",
    "    for k in top_k:\n",
    "      if output[0, 0, k].item() > 0.0:\n",
    "        log_k = math.log(output[0, 0, k].item())\n",
    "      else:\n",
    "        log_k = -sys.float_info.max\n",
    "\n",
    "      if k == end_token_id_fr:\n",
    "        ended_branches.append({\n",
    "          \"decoder_input\": {\n",
    "            \"input_encoded_en\": encoded_en_test,\n",
    "            \"input_fr\": np.array([[k]]),\n",
    "            \"input_h\": state_h_en_test,\n",
    "            \"input_c\": state_c_en_test,\n",
    "          },\n",
    "          \"score\": beam[\"score\"] + log_k,\n",
    "          \"parent_node\": beam,\n",
    "          \"depth\": beam[\"depth\"] + 1,\n",
    "          \"attention_weights\": attention_out,\n",
    "        })\n",
    "      else:\n",
    "        new_beam_candidates.append({\n",
    "          \"decoder_input\": {\n",
    "            \"input_encoded_en\": encoded_en_test,\n",
    "            \"input_fr\": np.array([[k]]),\n",
    "            \"input_h\": state_h_en_test,\n",
    "            \"input_c\": state_c_en_test,\n",
    "          },\n",
    "          \"score\": beam[\"score\"] + log_k,\n",
    "          \"parent_node\": beam,\n",
    "          \"depth\": beam[\"depth\"] + 1,\n",
    "          \"attention_weights\": attention_out,\n",
    "        })\n",
    "\n",
    "  # Keeping only the top beam_size in the list\n",
    "  beam_search_list = sorted(new_beam_candidates,\n",
    "                            key=lambda b: b[\"score\"],\n",
    "                            reverse=True)[0:beam_size]\n",
    "\n",
    "# Now that we are done with our beam search, let's take the best score and\n",
    "# detokenize it\n",
    "beam_node = sorted(beam_search_list + ended_branches,\n",
    "                   key=lambda b: b[\"score\"] / b[\"depth\"],\n",
    "                   reverse=True)[0]\n",
    "\n",
    "# Trace the best beam back to the parent node\n",
    "all_french_tokens = []\n",
    "attention_weights = []\n",
    "while beam_node[\"parent_node\"] is not None:\n",
    "    all_french_tokens.append(\n",
    "        beam_node[\"decoder_input\"][\"input_fr\"][0, 0].item())\n",
    "    attention_weights.append(beam_node[\"attention_weights\"])\n",
    "    beam_node = beam_node[\"parent_node\"]\n",
    "\n",
    "# We traced from tail to head, so we need to reserve the order to have it the right way\n",
    "all_french_tokens.reverse()\n",
    "attention_weights.reverse()\n",
    "\n",
    "# If there's any token out of the vocab, exclude it. This includes `<end>`,\n",
    "# `<empty>`, and <start> tokens\n",
    "french_tokens = [t for t in all_french_tokens if t < sp_fr.get_piece_size()]\n",
    "\n",
    "# Voila!\n",
    "french_string = sp_fr.DecodeIds(french_tokens)\n",
    "\n",
    "print(\"The input English string: \", english_string)\n",
    "print(\"The output French string: \", french_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the alignment matrix\n",
    "attention_mat = []\n",
    "for attn in attention_weights:\n",
    "  attention_mat.append(attn.reshape(-1))\n",
    "\n",
    "# We want to have the English tokens on the left axis, so we need to\n",
    "# trasponse the matrix over the diagonal running from upper right to lower left\n",
    "attention_mat = np.flipud(np.transpose(np.flipud(attention_mat)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.imshow(attention_mat)\n",
    "\n",
    "ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "def map_en_special_tokens(t):\n",
    "    switcher = {}\n",
    "    switcher[end_token_id_en] = \"<end>\"\n",
    "    switcher[empty_token_id_en] = \"<empty>\"\n",
    "    return switcher.get(t, \"<unknown>\")\n",
    "\n",
    "def map_fr_special_tokens(t):\n",
    "    switcher = {}\n",
    "    switcher[end_token_id_fr] = \"<end>\"\n",
    "    switcher[empty_token_id_fr] = \"<empty>\"\n",
    "    switcher[start_token_id_fr] = \"<start>\"\n",
    "    return switcher.get(t, \"<unknown>\")\n",
    "\n",
    "ax.set_xticklabels([sp_fr.IdToPiece(t)\n",
    "                    if t < sp_fr.get_piece_size() else map_fr_special_tokens(t)\n",
    "                    for t in all_french_tokens])\n",
    "ax.set_yticklabels([sp_en.IdToPiece(t)\n",
    "                    if t < sp_en.get_piece_size() else map_en_special_tokens(t)\n",
    "                    for t in english_tokens[0].tolist()])\n",
    "\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# translate(u'I am going to work.')\n",
    "# translate(u'The project is super hard.')\n",
    "# util.translate(u'She works at home.', units, max_target_length, max_source_length, encoder, decoder, source_tokenizer, target_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tfgpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a75d68007abe45c64e50bc70737987651d46d0fb341f8ff51c4351246529ca1b"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}