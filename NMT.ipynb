{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.attention as attention\n",
    "importlib.reload(attention);\n",
    "import src.models as model\n",
    "importlib.reload(model)\n",
    "import src.utils as util\n",
    "importlib.reload(util);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-word mapping\n",
    "target_vocab_size_en = 400\n",
    "target_vocab_size_fr = 600\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\" --input=data/small_vocab_en --model_type=unigram --hard_vocab_limit=false\" +\n",
    "    f\" --model_prefix=data/en --vocab_size={target_vocab_size_en}\")\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\" --input=data/small_vocab_fr --model_type=unigram --hard_vocab_limit=false\" +\n",
    "    f\" --model_prefix=data/fr --vocab_size={target_vocab_size_fr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global data\n",
    "sp_en = spm.SentencePieceProcessor()\n",
    "sp_en.Load(os.path.join(\"data\", 'en.model'))\n",
    "\n",
    "sp_fr = spm.SentencePieceProcessor()\n",
    "sp_fr.Load(os.path.join(\"data\", 'fr.model'))\n",
    "\n",
    "\n",
    "file = open(os.path.join('data', 'small_vocab_en'), 'r', encoding='utf-8')\n",
    "en_text = file.read().split(\"\\n\")\n",
    "file = open(os.path.join('data', 'small_vocab_fr'), 'r', encoding='utf-8')\n",
    "fr_text = file.read().split(\"\\n\")\n",
    "\n",
    "train_en_X = []\n",
    "train_fr_X = []\n",
    "train_fr_Y = []\n",
    "\n",
    "en_max_len = 0\n",
    "fr_max_len = 0\n",
    "\n",
    "vocab_size_en = sp_en.GetPieceSize()\n",
    "vocab_size_fr = sp_fr.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Tokenizer\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bert(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "# source_tokenizer = [tokenize_bert(ele) for ele in source]\n",
    "# source_tensor = source_tokenizer\n",
    "# print(source_tokenizer[0])\n",
    "\n",
    "# target_tokenizer = [tokenize_bert(ele) for ele in target]\n",
    "# target_tensor = target_tokenizer\n",
    "# print(target_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(source_tensor, target_tensor, test_size=0.2, random_state=0)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "# Show length\n",
    "# print(len(X_train), len(y_train), len(X_val), len(y_val))\n",
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the source and target tokens and post pad them\n",
    "# Assuming three extra tokens: <end>: #vocab_size_en | #vocab_size_fr,\n",
    "# <empty>: #vocab_size_en+1 | #vocab_size_fr+1, and <start>: #vocab_size_fr+2\n",
    "\n",
    "end_token_id_en = vocab_size_en\n",
    "empty_token_id_en = vocab_size_en + 1\n",
    "end_token_id_fr = vocab_size_fr\n",
    "empty_token_id_fr = vocab_size_fr + 1\n",
    "start_token_id_fr = vocab_size_fr + 2\n",
    "\n",
    "# The input text only needs two extra tokens while the output needs 3\n",
    "vocab_size_en = vocab_size_en + 2\n",
    "vocab_size_fr = vocab_size_fr + 3\n",
    "\n",
    "\n",
    "for i in range(len(en_text)):\n",
    "  en_seq = sp_en.EncodeAsIds(en_text[i].strip()) + [end_token_id_en]\n",
    "  en_max_len = max(en_max_len, len(en_seq))\n",
    "  train_en_X.append(en_seq)\n",
    "\n",
    "  fr_seq = sp_fr.EncodeAsIds(fr_text[i].strip()) + [end_token_id_fr]\n",
    "  fr_max_len = max(fr_max_len, len(fr_seq))\n",
    "  train_fr_X.append(fr_seq)\n",
    "\n",
    "# Cleaning up the memory, work with subword\n",
    "en_text = []\n",
    "fr_text = []\n",
    "\n",
    "# Padding all the samples with <empty> token to make them all of the same length\n",
    "# equal to the longest one\n",
    "train_en_X = pad_sequences(train_en_X, maxlen=en_max_len,\n",
    "                           padding=\"post\", value=empty_token_id_en)\n",
    "# maxlen is fr_max_len+1 since we need to accomodate for <start>\n",
    "train_fr_X = pad_sequences(train_fr_X, maxlen=fr_max_len+1,\n",
    "                           padding=\"post\", value=empty_token_id_fr)\n",
    "\n",
    "# Converting the train_fr_Y to a one-hot vector needed by the training phase as\n",
    "# the output\n",
    "train_fr_Y = to_categorical(train_fr_X, num_classes=vocab_size_fr)\n",
    "\n",
    "# Moving the last <empty> to the first position in each input sample\n",
    "train_fr_X = np.roll(train_fr_X, 1, axis=-1)\n",
    "# Changing the first token in each input sample to <start>\n",
    "train_fr_X[:, 0] = start_token_id_fr\n",
    "\n",
    "fr_max_len = fr_max_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "util.convert(source_tokenizer, X_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "util.convert(target_tokenizer, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data dataset\n",
    "hidden_dim = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder input (English)\n",
    "input_en = Input(batch_shape=(None, en_max_len), name='input_en')\n",
    "\n",
    "# English embedding layer\n",
    "embedding_en = layers.Embedding(vocab_size_en, hidden_dim, name='embedding_en')\n",
    "embedded_en = embedding_en(input_en)\n",
    "\n",
    "# Encoder RNN (LSTM) layer\n",
    "encoder_lstm = layers.Bidirectional(\n",
    "                  layers.LSTM(hidden_dim,\n",
    "                              return_sequences=True, return_state=True),\n",
    "                  name=\"encoder_lstm\")\n",
    "(encoded_en,\n",
    "  forward_h_en, forward_c_en,\n",
    "  backward_h_en, backward_c_en) = encoder_lstm(embedded_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder input (French)\n",
    "input_fr = Input(batch_shape=(None, None), name='input_fr')\n",
    "\n",
    "# English embedding layer\n",
    "embedding_fr = layers.Embedding(vocab_size_fr, hidden_dim, name='embedding_fr')\n",
    "embedded_fr = embedding_fr(input_fr)\n",
    "\n",
    "state_h_en = layers.concatenate([forward_h_en, backward_h_en])\n",
    "state_c_en = layers.concatenate([forward_c_en, backward_c_en])\n",
    "\n",
    "# Decoder RNN (LSTM) layer\n",
    "decoder_lstm = layers.LSTM(hidden_dim * 2, return_sequences=True,\n",
    "                           return_state=True, name=\"decoder_lstm\")\n",
    "(encoded_fr,\n",
    "  forward_h_fr, forward_c_fr) = decoder_lstm(embedded_fr,\n",
    "                 initial_state=[state_h_en, state_c_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention layer\n",
    "attention_layer = attention.AttentionLayer(name='attention_layer')\n",
    "attention_out, attention_states = attention_layer({\"values\": encoded_en,\n",
    "                                                   \"query\": encoded_fr})\n",
    "\n",
    "# Concatenating the decoder output with attention output\n",
    "rnn_output = layers.concatenate([encoded_fr, attention_out], name=\"rnn_output\")\n",
    "\n",
    "# Dense layer\n",
    "dense_layer0 = layers.Dense(2048, activation='relu', name='dense_0')\n",
    "dl0 = dense_layer0(rnn_output)\n",
    "\n",
    "dense_layer1 = layers.Dense(1024, activation='relu', name='dense_1')\n",
    "dl1 = dense_layer1(dl0)\n",
    "\n",
    "dense_layer2 = layers.Dense(512, activation='relu', name='dense_2')\n",
    "dl2 = dense_layer2(dl1)\n",
    "\n",
    "dl2 = layers.Dropout(0.4)(dl2)\n",
    "\n",
    "dense_layer3 = layers.Dense(vocab_size_fr, activation='softmax', name='dense_3')\n",
    "dense_output = dense_layer3(dl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "training_model = models.Model([input_en, input_fr], dense_output)\n",
    "training_model.summary()\n",
    "\n",
    "training_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=[model.MaskedCategoricalAccuracy(empty_token_id_fr),\n",
    "                                model.ExactMatchedAccuracy(empty_token_id_fr)])\n",
    "\n",
    "# Generative models\n",
    "encoder_model = models.Model([input_en],\n",
    "                             [encoded_en,\n",
    "                              state_h_en, state_c_en])\n",
    "encoder_model.summary()\n",
    "\n",
    "\n",
    "# The decoder model, to generate the French tokens (in integer form)\n",
    "input_h = layers.Input(batch_shape=(None, hidden_dim * 2),\n",
    "                       name='input_h')\n",
    "input_c = layers.Input(batch_shape=(None, hidden_dim * 2),\n",
    "                       name='input_c')\n",
    "\n",
    "(decoder_output,\n",
    "  output_h,\n",
    "  output_c) = decoder_lstm(embedded_fr,\n",
    "                           initial_state=[input_h, input_c])\n",
    "\n",
    "input_encoded_en = layers.Input(batch_shape=(None, en_max_len, hidden_dim * 2),\n",
    "                                name='input_encoded_en')\n",
    "\n",
    "attention_out, attention_state = attention_layer({\"values\": input_encoded_en,\n",
    "                                                  \"query\": decoder_output})\n",
    "\n",
    "generative_output = layers.concatenate([decoder_output,\n",
    "                                        attention_out],\n",
    "                                       name=\"generative_output\")\n",
    "\n",
    "g0 = dense_layer0(generative_output)\n",
    "g1 = dense_layer1(g0)\n",
    "g2 = dense_layer2(g1)\n",
    "dense_output = dense_layer3(g2)\n",
    "\n",
    "decoder_model = models.Model([input_encoded_en, input_fr,\n",
    "                              input_h, input_c],\n",
    "                             [dense_output, attention_state,\n",
    "                              output_h, output_c])\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "pocket = EarlyStopping(monitor='val_exact_matched_accuracy', min_delta=0.001,\n",
    "                       patience=10, verbose=1, mode='max',\n",
    "                       restore_best_weights = True)\n",
    "\n",
    "history = training_model.fit(x=[train_en_X, train_fr_X], y=train_fr_Y, batch_size=786,\n",
    "                             epochs=100, verbose=1, validation_split=0.3, shuffle=True,\n",
    "                             workers=3, use_multiprocessing=True, callbacks=[pocket])\n",
    "\n",
    "training_model.save_weights(os.path.join(\"data\", \"nmt.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "training_model.load_weights(\n",
    "    os.path.join(\"data\", \"nmt.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "util.plot_training(history)\n",
    "\n",
    "results = training_model.evaluate(x=[train_en_X, train_fr_X], y=train_fr_Y,\n",
    "                                  batch_size=786, verbose=1,\n",
    "                                  workers=1, use_multiprocessing=False)\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test masked categorical accuracy:', results[1])\n",
    "print('Test exact matched accuracy:', results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# The input English string\n",
    "english_string = \"the united states is never freezing during november , but the united states is sometimes rainy in winter .\"\n",
    "\n",
    "# First, let's tokenize the Eglish string, then pad it\n",
    "english_tokens = sp_en.EncodeAsIds(english_string.strip()) + [end_token_id_en]\n",
    "english_tokens = pad_sequences([english_tokens], maxlen=en_max_len,\n",
    "                               padding=\"post\", value=empty_token_id_en)\n",
    "\n",
    "# The encoder, we only need to use it once per each English string\n",
    "(encoded_en_test,\n",
    "  state_h_en_test, state_c_en_test) = encoder_model.predict(english_tokens)\n",
    "\n",
    "# In order to find a better translation, we are using Beam search\n",
    "beam_search_list = [{\n",
    "  \"decoder_input\": {\n",
    "    \"input_encoded_en\": encoded_en_test,\n",
    "    \"input_fr\": np.array([[start_token_id_fr]]),\n",
    "    \"input_h\": state_h_en_test,\n",
    "    \"input_c\": state_c_en_test\n",
    "  },\n",
    "  \"score\": 0.0,\n",
    "  \"parent_node\": None,\n",
    "  \"depth\": 0,\n",
    "  \"attention_weights\": None,\n",
    "}]\n",
    "ended_branches = []\n",
    "\n",
    "beam_size = 10\n",
    "\n",
    "# We are generating up to fr_max_len tokens\n",
    "for i in range(fr_max_len):\n",
    "  new_beam_candidates = []\n",
    "  # Predict the next token for each member of the list\n",
    "  for beam in beam_search_list:\n",
    "    # Use the decoder to predict the next token using the previously\n",
    "    # predicted token\n",
    "    (output,\n",
    "      attention_out,\n",
    "      state_h_en_test,\n",
    "      state_c_en_test) = decoder_model.predict(beam[\"decoder_input\"])\n",
    "    # Find the top beam_size candidates\n",
    "    top_k = np.argpartition(output[0, 0, :], -beam_size)[-beam_size:]\n",
    "    # For each candidate, put it in the list to predict the next token for it\n",
    "    for k in top_k:\n",
    "      if output[0, 0, k].item() > 0.0:\n",
    "        log_k = math.log(output[0, 0, k].item())\n",
    "      else:\n",
    "        log_k = -sys.float_info.max\n",
    "\n",
    "      if k == end_token_id_fr:\n",
    "        ended_branches.append({\n",
    "          \"decoder_input\": {\n",
    "            \"input_encoded_en\": encoded_en_test,\n",
    "            \"input_fr\": np.array([[k]]),\n",
    "            \"input_h\": state_h_en_test,\n",
    "            \"input_c\": state_c_en_test,\n",
    "          },\n",
    "          \"score\": beam[\"score\"] + log_k,\n",
    "          \"parent_node\": beam,\n",
    "          \"depth\": beam[\"depth\"] + 1,\n",
    "          \"attention_weights\": attention_out,\n",
    "        })\n",
    "      else:\n",
    "        new_beam_candidates.append({\n",
    "          \"decoder_input\": {\n",
    "            \"input_encoded_en\": encoded_en_test,\n",
    "            \"input_fr\": np.array([[k]]),\n",
    "            \"input_h\": state_h_en_test,\n",
    "            \"input_c\": state_c_en_test,\n",
    "          },\n",
    "          \"score\": beam[\"score\"] + log_k,\n",
    "          \"parent_node\": beam,\n",
    "          \"depth\": beam[\"depth\"] + 1,\n",
    "          \"attention_weights\": attention_out,\n",
    "        })\n",
    "\n",
    "  # Keeping only the top beam_size in the list\n",
    "  beam_search_list = sorted(new_beam_candidates,\n",
    "                            key=lambda b: b[\"score\"],\n",
    "                            reverse=True)[0:beam_size]\n",
    "\n",
    "# Now that we are done with our beam search, let's take the best score and\n",
    "# detokenize it\n",
    "beam_node = sorted(beam_search_list + ended_branches,\n",
    "                   key=lambda b: b[\"score\"] / b[\"depth\"],\n",
    "                   reverse=True)[0]\n",
    "\n",
    "# Trace the best beam back to the parent node\n",
    "all_french_tokens = []\n",
    "attention_weights = []\n",
    "while beam_node[\"parent_node\"] is not None:\n",
    "    all_french_tokens.append(\n",
    "        beam_node[\"decoder_input\"][\"input_fr\"][0, 0].item())\n",
    "    attention_weights.append(beam_node[\"attention_weights\"])\n",
    "    beam_node = beam_node[\"parent_node\"]\n",
    "\n",
    "# We traced from tail to head, so we need to reserve the order to have it the right way\n",
    "all_french_tokens.reverse()\n",
    "attention_weights.reverse()\n",
    "\n",
    "# If there's any token out of the vocab, exclude it. This includes `<end>`,\n",
    "# `<empty>`, and <start> tokens\n",
    "french_tokens = [t for t in all_french_tokens if t < sp_fr.get_piece_size()]\n",
    "\n",
    "# Voila!\n",
    "french_string = sp_fr.DecodeIds(french_tokens)\n",
    "\n",
    "print(\"The input English string: \", english_string)\n",
    "print(\"The output French string: \", french_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the alignment matrix\n",
    "attention_mat = []\n",
    "for attn in attention_weights:\n",
    "  attention_mat.append(attn.reshape(-1))\n",
    "\n",
    "# We want to have the English tokens on the left axis, so we need to\n",
    "# trasponse the matrix over the diagonal running from upper right to lower left\n",
    "attention_mat = np.flipud(np.transpose(np.flipud(attention_mat)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.imshow(attention_mat)\n",
    "\n",
    "ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "def map_en_special_tokens(t):\n",
    "    switcher = {}\n",
    "    switcher[end_token_id_en] = \"<end>\"\n",
    "    switcher[empty_token_id_en] = \"<empty>\"\n",
    "    return switcher.get(t, \"<unknown>\")\n",
    "\n",
    "def map_fr_special_tokens(t):\n",
    "    switcher = {}\n",
    "    switcher[end_token_id_fr] = \"<end>\"\n",
    "    switcher[empty_token_id_fr] = \"<empty>\"\n",
    "    switcher[start_token_id_fr] = \"<start>\"\n",
    "    return switcher.get(t, \"<unknown>\")\n",
    "\n",
    "ax.set_xticklabels([sp_fr.IdToPiece(t)\n",
    "                    if t < sp_fr.get_piece_size() else map_fr_special_tokens(t)\n",
    "                    for t in all_french_tokens])\n",
    "ax.set_yticklabels([sp_en.IdToPiece(t)\n",
    "                    if t < sp_en.get_piece_size() else map_en_special_tokens(t)\n",
    "                    for t in english_tokens[0].tolist()])\n",
    "\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# translate(u'I am going to work.')\n",
    "# translate(u'The project is super hard.')\n",
    "# util.translate(u'She works at home.', units, max_target_length, max_source_length, encoder, decoder, source_tokenizer, target_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('testenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "65a42bdb2f4fd1e2963f63fbecc12d40428c78ad536e43f0559247f991c7d5d2"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}